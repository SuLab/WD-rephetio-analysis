{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying WikiData for henet edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "import wdhetnetbuilder as wdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_info_dir = Path('../0_data/manual').resolve()\n",
    "h = wdh.WDHetnetQueryBuilder(net_info_dir.joinpath('node_info.json'),\n",
    "                             net_info_dir.joinpath('edge_info.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the strucure of the metagraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetnet_edges = [\n",
    "    {'abbrev': 'CdiC'},\n",
    "    {'abbrev': 'CtD'},\n",
    "    #{'abbrev': 'PPaiC'},\n",
    "    {'abbrev': 'CHhcC'},\n",
    "    {'abbrev': 'PWhpC'},\n",
    "    {'abbrev': 'CpP'},\n",
    "    {'abbrev': 'PiwC'},\n",
    "    {'abbrev': 'VntC'},\n",
    "    {'abbrev': 'VptC'},\n",
    "    {'abbrev': 'DaP', 'target': 'Gene'},\n",
    "    {'abbrev': 'DaG'},\n",
    "    {'abbrev': 'DsyS'},\n",
    "    {'abbrev': 'DmsMS'},\n",
    "    {'abbrev': 'CHsyS'},\n",
    "    {'abbrev': 'CHsyD'},\n",
    "    {'abbrev': 'VndD'},\n",
    "    {'abbrev': 'VpdD'},\n",
    "    {'abbrev': 'VvP', 'target': 'Gene'},\n",
    "    {'abbrev': 'VvG'},\n",
    "    {'abbrev': 'PWhpP', 'target': 'Gene'},\n",
    "    {'abbrev': 'PWhpG'},\n",
    "    {'abbrev': 'PccCC'},\n",
    "    {'abbrev': 'PbpBP'},\n",
    "    {'abbrev': 'PmfMF'},\n",
    "    {'abbrev': 'PhpPD'},\n",
    "    {'abbrev': 'PhpSS'},\n",
    "    {'abbrev': 'PFhpP'},\n",
    "    {'abbrev': 'PhpBS'},\n",
    "    {'abbrev': 'PhpAS'},\n",
    "    {'abbrev': 'PhpSM'},\n",
    "    #{'abbrev': 'PPtaD'},\n",
    "    {'abbrev': 'CrCR'},\n",
    "    {'abbrev': 'DlA'},\n",
    "    {'abbrev': 'CHafA'},\n",
    "    {'abbrev': 'CtCH'},\n",
    "    {'abbrev': 'BPhpC'},\n",
    "    {'abbrev': 'PccA'},\n",
    "    {'abbrev': 'PWhpBP'},\n",
    "    {'abbrev': 'PFhpBS'},\n",
    "    {'abbrev': 'PDhpSS'},\n",
    "    {'abbrev': 'PFhpSS'},\n",
    "    {'abbrev': 'PWhpBP'},\n",
    "    {'abbrev': 'PFhpPD'},\n",
    "    {'abbrev': 'PFhpAS'},\n",
    "    {'abbrev': 'PregBP'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [h.build_query_from_abbrev(**edge) for edge in hetnet_edges]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error was found in the Feburary 2018 Data Dump... The majority of Biological Process nodes are missing their `instance_of Biological Process` statment (`wdt:P31 'wd:Q996394`), leading to severely decreased number of edges with these node types.  \n",
    "\n",
    "Because biological processes are also defined by the property `Biological Process` (`wdt:P686`) we can use this as well as a check for a GO Term Identifier to recover these edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_queries_2_2018 = [h.build_query_from_abbrev(**edge) for edge in hetnet_edges]\n",
    "\n",
    "# Biological Process nodes forwhatever reason lost their wdt:P31 wd:Q2996394 statments in 2018 for whatever reason\n",
    "# so instead still use the biological process proterty (wdt:P682) beteen the protien and bp \n",
    "# and check to make sure they have a go id... (wdt:P686)\n",
    "queries_2_2018 = []\n",
    "for q in ini_queries_2_2018:\n",
    "    queries_2_2018.append(q.replace(\"\"\"    ?biological_process wdt:P31 wd:Q2996394 .\"\"\", \n",
    "                                    \"\"\"    ?biological_process wdt:P686 ?go_id .\"\"\")\n",
    "                           .replace(\"\"\"    ?biological_process1 wdt:P31 wd:Q2996394 .\"\"\",\n",
    "                                    \"\"\"    ?biological_process1 wdt:P686 ?go_id1 .\"\"\")\n",
    "                           .replace(\"\"\"    ?biological_process2 wdt:P31 wd:Q2996394 .\"\"\", \n",
    "                                    \"\"\"    ?biological_process2 wdt:P686 ?go_id2 .\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar problem was found back in early 2017: Genes and proteins were `subclass of` Gene or Protein... not `instance of`...  Disease was a mess, with some `subclass of` some `instance of` and some both... fixing these for our 2017 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix gene and protein\n",
    "h.node_info['Gene']['subclass'] = True\n",
    "h.node_info['Protein']['subclass'] = True\n",
    "\n",
    "# Update the class with the new info \n",
    "# TODO: Add an update node method that re-runs this auto-magically...\n",
    "h.subclass = h._extract_node_key('subclass')\n",
    "h.extend = h._extract_node_key('extend')\n",
    "\n",
    "ini_queries_2017 = [h.build_query_from_abbrev(**edge) for edge in hetnet_edges]\n",
    "\n",
    "# Disease are sometimes 'instance_of', sometimes 'subclass_of', so we will ectend both...\n",
    "queries_2017 = []\n",
    "for q in ini_queries_2017:\n",
    "    queries_2017.append(q.replace(\"\"\"    # Initial typing for Disease\n",
    "    ?disease wdt:P31 wd:Q12136 .\"\"\", \"\"\"    # Initial typing for Disease\n",
    "    ?disease wdt:P31|wdt:P279* wd:Q12136 .\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT ?compound ?compoundLabel ?disease ?diseaseLabel \n",
      "WHERE {\n",
      "\n",
      "    # Initial typing for Compound\n",
      "    ?compound wdt:P31 wd:Q11173 .\n",
      "\n",
      "    # Initial typing for Disease\n",
      "    ?disease wdt:P31 wd:Q12136 .\n",
      "\n",
      "    { ?compound wdt:P2175/^wdt:P279? ?disease }\n",
      "    UNION { ?disease wdt:P279?/wdt:P2176 ?compound }\n",
      "\n",
      "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGAGE],en\" }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(h.build_query_from_abbrev('CtD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = {\n",
    "    'https://query.wikidata.org/sparql': datetime.today().strftime('%Y-%m-%d'),\n",
    "    'http://avalanche.scripps.edu:9988/bigdata/sparql': '2018-11-12',\n",
    "    'http://avalanche.scripps.edu:9999/bigdata/sparql': '2018-02-05',\n",
    "    'http://kylo.scripps.edu:9988/bigdata/sparql': '2017-01-16',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd761dc7ee849b184e55a53645f54ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='All Endpoints', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579f3287f65a4006a91ec23ecea04a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2018-11-12 Data', max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf78a806320b4996930e5f441f20f953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2018-02-05 Data', max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682a2b80a53d4945a0d7f0afe5df6c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2017-01-16 Data', max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68b8ab7a91449db8df31e5db901af4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2019-09-13 Data', max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "# Sort so live wikidata is done last incase of errors on local instances...\n",
    "for ep, dump_date in tqdm_notebook(sorted(endpoints.items()), desc='All Endpoints'):\n",
    "\n",
    "    # Get the correct set of queries for the correct years...\n",
    "    if dump_date.startswith('2017'):\n",
    "        to_query = queries_2017\n",
    "    elif dump_date.startswith('2018-02'):\n",
    "        to_query = queries_2_2018\n",
    "    else:\n",
    "        to_query = queries\n",
    "    \n",
    "    cur_res = dict()\n",
    "    for meta_edge, query in tqdm_notebook(zip(hetnet_edges, to_query),\n",
    "                                            desc=dump_date+' Data',\n",
    "                                            total=len(hetnet_edges)):\n",
    "\n",
    "        cur_res[meta_edge['abbrev']] = wdh.execute_sparql_query(query, endpoint=ep)\n",
    "\n",
    "    results[dump_date] = cur_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2018-11-12</th>\n",
       "      <th>2018-02-05</th>\n",
       "      <th>2017-01-16</th>\n",
       "      <th>2019-09-13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CdiC</th>\n",
       "      <td>1807</td>\n",
       "      <td>1807</td>\n",
       "      <td>1798</td>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CtD</th>\n",
       "      <td>35204</td>\n",
       "      <td>16508</td>\n",
       "      <td>21679</td>\n",
       "      <td>37160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHhcC</th>\n",
       "      <td>576</td>\n",
       "      <td>582</td>\n",
       "      <td>604</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PWhpC</th>\n",
       "      <td>3316</td>\n",
       "      <td>2896</td>\n",
       "      <td>0</td>\n",
       "      <td>3637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CpP</th>\n",
       "      <td>3874</td>\n",
       "      <td>3857</td>\n",
       "      <td>3509</td>\n",
       "      <td>3841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiwC</th>\n",
       "      <td>3634</td>\n",
       "      <td>3619</td>\n",
       "      <td>3659</td>\n",
       "      <td>3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VntC</th>\n",
       "      <td>447</td>\n",
       "      <td>145</td>\n",
       "      <td>90</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VptC</th>\n",
       "      <td>633</td>\n",
       "      <td>532</td>\n",
       "      <td>134</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DaP</th>\n",
       "      <td>2130</td>\n",
       "      <td>2140</td>\n",
       "      <td>2964</td>\n",
       "      <td>9368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DaG</th>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DsyS</th>\n",
       "      <td>300</td>\n",
       "      <td>227</td>\n",
       "      <td>71</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DmsMS</th>\n",
       "      <td>6506</td>\n",
       "      <td>4676</td>\n",
       "      <td>2649</td>\n",
       "      <td>6807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHsyS</th>\n",
       "      <td>3204</td>\n",
       "      <td>3251</td>\n",
       "      <td>1658</td>\n",
       "      <td>2990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHsyD</th>\n",
       "      <td>1463</td>\n",
       "      <td>1372</td>\n",
       "      <td>2273</td>\n",
       "      <td>1477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VndD</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VpdD</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VvP</th>\n",
       "      <td>1758</td>\n",
       "      <td>1640</td>\n",
       "      <td>422</td>\n",
       "      <td>2659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VvG</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PWhpP</th>\n",
       "      <td>17957</td>\n",
       "      <td>15170</td>\n",
       "      <td>0</td>\n",
       "      <td>27865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PWhpG</th>\n",
       "      <td>419</td>\n",
       "      <td>517</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PccCC</th>\n",
       "      <td>93675</td>\n",
       "      <td>92194</td>\n",
       "      <td>2326</td>\n",
       "      <td>107158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PbpBP</th>\n",
       "      <td>153467</td>\n",
       "      <td>148448</td>\n",
       "      <td>139</td>\n",
       "      <td>172582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PmfMF</th>\n",
       "      <td>81079</td>\n",
       "      <td>75400</td>\n",
       "      <td>0</td>\n",
       "      <td>90464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhpPD</th>\n",
       "      <td>28416</td>\n",
       "      <td>27928</td>\n",
       "      <td>50004</td>\n",
       "      <td>30793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhpSS</th>\n",
       "      <td>5823</td>\n",
       "      <td>5818</td>\n",
       "      <td>4378</td>\n",
       "      <td>6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFhpP</th>\n",
       "      <td>45985</td>\n",
       "      <td>45572</td>\n",
       "      <td>60</td>\n",
       "      <td>117018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhpBS</th>\n",
       "      <td>810</td>\n",
       "      <td>808</td>\n",
       "      <td>926</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhpAS</th>\n",
       "      <td>987</td>\n",
       "      <td>984</td>\n",
       "      <td>1144</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhpSM</th>\n",
       "      <td>2094</td>\n",
       "      <td>2091</td>\n",
       "      <td>2522</td>\n",
       "      <td>2258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CrCR</th>\n",
       "      <td>9899</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>9829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DlA</th>\n",
       "      <td>757</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHafA</th>\n",
       "      <td>1517</td>\n",
       "      <td>1271</td>\n",
       "      <td>1278</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CtCH</th>\n",
       "      <td>45</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPhpC</th>\n",
       "      <td>9386</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PccA</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>17617</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PWhpBP</th>\n",
       "      <td>12445</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFhpBS</th>\n",
       "      <td>302</td>\n",
       "      <td>298</td>\n",
       "      <td>323</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDhpSS</th>\n",
       "      <td>442</td>\n",
       "      <td>441</td>\n",
       "      <td>609</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFhpSS</th>\n",
       "      <td>2454</td>\n",
       "      <td>2450</td>\n",
       "      <td>2203</td>\n",
       "      <td>2470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFhpPD</th>\n",
       "      <td>8085</td>\n",
       "      <td>8079</td>\n",
       "      <td>10630</td>\n",
       "      <td>8430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFhpAS</th>\n",
       "      <td>390</td>\n",
       "      <td>385</td>\n",
       "      <td>402</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PregBP</th>\n",
       "      <td>42097</td>\n",
       "      <td>39759</td>\n",
       "      <td>0</td>\n",
       "      <td>42655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2018-11-12  2018-02-05  2017-01-16  2019-09-13\n",
       "CdiC          1807        1807        1798        1720\n",
       "CtD          35204       16508       21679       37160\n",
       "CHhcC          576         582         604         571\n",
       "PWhpC         3316        2896           0        3637\n",
       "CpP           3874        3857        3509        3841\n",
       "PiwC          3634        3619        3659        3607\n",
       "VntC           447         145          90         602\n",
       "VptC           633         532         134         793\n",
       "DaP           2130        2140        2964        9368\n",
       "DaG             31          36          28          34\n",
       "DsyS           300         227          71         362\n",
       "DmsMS         6506        4676        2649        6807\n",
       "CHsyS         3204        3251        1658        2990\n",
       "CHsyD         1463        1372        2273        1477\n",
       "VndD             6           0           4           9\n",
       "VpdD            69           1          42          83\n",
       "VvP           1758        1640         422        2659\n",
       "VvG              3           3           0           3\n",
       "PWhpP        17957       15170           0       27865\n",
       "PWhpG          419         517           0         548\n",
       "PccCC        93675       92194        2326      107158\n",
       "PbpBP       153467      148448         139      172582\n",
       "PmfMF        81079       75400           0       90464\n",
       "PhpPD        28416       27928       50004       30793\n",
       "PhpSS         5823        5818        4378        6409\n",
       "PFhpP        45985       45572          60      117018\n",
       "PhpBS          810         808         926         861\n",
       "PhpAS          987         984        1144        1055\n",
       "PhpSM         2094        2091        2522        2258\n",
       "CrCR          9899          71           1        9829\n",
       "DlA            757         101          76         724\n",
       "CHafA         1517        1271        1278        1256\n",
       "CtCH            45          56          56          45\n",
       "BPhpC         9386           1           0        9298\n",
       "PccA            51           0       17617        1844\n",
       "PWhpBP       12445          14           0       12445\n",
       "PFhpBS         302         298         323         301\n",
       "PDhpSS         442         441         609         441\n",
       "PFhpSS        2454        2450        2203        2470\n",
       "PFhpPD        8085        8079       10630        8430\n",
       "PFhpAS         390         385         402         395\n",
       "PregBP       42097       39759           0       42655"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_count = []\n",
    "for date, res in results.items():\n",
    "    counts = pd.Series({name: len(res[name]) for name in res}, name=date)\n",
    "    edge_count.append(counts)\n",
    "edge_count = pd.concat(edge_count, axis=1)\n",
    "edge_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_name = '01_querying_wikidata_for_hetnet_edges'\n",
    "out_dir = Path('../2_pipeline').resolve().joinpath(this_name, 'out')\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "edge_count.to_csv(out_dir.joinpath('edge_counts.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Error Fixing\n",
    "\n",
    "1. If start and end nodetypes are the same, could potentiall have node_id1 -> node_id2 and node_id2 -> node_id1... This is only useful if the edge is directed, but most of these edges are bi-directional (undirected) so only one of the directions is needed.\n",
    "\n",
    "2. Since WikiData can have more than one 'instance_of' statment per node, some nodes may be members of mulitple types... will look at those queried and see where they are.\n",
    "\n",
    "3. Qualified statments need further processing, so we will collect those\n",
    "\n",
    "4. Multi-step edges that will be compresssed to 1 edge need further processing, so we will collect those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_query_numb(query_name):\n",
    "    numb = wdh.get_query_numb(query_name)\n",
    "    if numb:\n",
    "        idx = query_name.index(numb)\n",
    "        return query_name[:idx]\n",
    "    else:\n",
    "        return query_name\n",
    "    \n",
    "def to_full_name(query_name):\n",
    "    name = remove_query_numb(query_name)\n",
    "    return name.replace('_', ' ').title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_res(q_result):\n",
    "\n",
    "    node_ids = dict()\n",
    "    id_to_name = dict()\n",
    "    self_ref = set()\n",
    "    qualified = set()\n",
    "    multi_step = set()\n",
    "\n",
    "    # Do some processing on the collected edges\n",
    "    for e, r in q_result.items():\n",
    "\n",
    "        s_kind, e_type, e_kind = wdh.gt.parse_edge_abbrev(e)\n",
    "        all_n_types = [c for c in r.columns if not c.endswith('Label')]\n",
    "\n",
    "\n",
    "        for nt in all_n_types:\n",
    "            # Get the node type by removing any trailing numbers\n",
    "            numb = wdh.get_query_numb(nt)\n",
    "            if numb:\n",
    "                idx = nt.index(numb)\n",
    "                node_type = nt[:idx]\n",
    "            else:\n",
    "                node_type = nt\n",
    "\n",
    "            # For a given node type, collect all the ids... don't need qualifiers\n",
    "            if node_type != 'qualifier': \n",
    "                if node_type in node_ids:\n",
    "                    node_ids[node_type].update(set(r[nt]))\n",
    "                else:\n",
    "                    node_ids[node_type] = set(r[nt])\n",
    "                id_to_name.update(r.set_index(nt)[nt+'Label'].to_dict())\n",
    "\n",
    "            # Identifiy self_reffrenetial edges\n",
    "            if s_kind == e_kind:\n",
    "                self_ref.add(e)\n",
    "\n",
    "            if len(all_n_types) > 2:\n",
    "                # Grab qualified edges for further processing\n",
    "                if 'qualifier' in all_n_types:\n",
    "                    qualified.add(e)\n",
    "                # Currently, an edge can not be both multi-step and qualified\n",
    "                else:\n",
    "                    multi_step.add(e)\n",
    "                    \n",
    "    return node_ids, id_to_name, self_ref, qualified, multi_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_self_ref_edges(q_result, self_ref, id_to_name):\n",
    "    fixed = dict()\n",
    "\n",
    "    for kind in tqdm_notebook(self_ref):\n",
    "\n",
    "        # no need to worry about forward vs reverse in directed edges\n",
    "        if '>' in kind or '<' in kind:\n",
    "            continue\n",
    "\n",
    "        # Only look at 1 kind of edge at a time\n",
    "        this_edges = q_result[kind]\n",
    "        col_names = this_edges.columns\n",
    "\n",
    "        edge_ids = set()\n",
    "\n",
    "        for row in this_edges.itertuples():\n",
    "            # Grab the edge ID, sorting, so lowest ID first:\n",
    "            #     If both 'Q00001 -- Q00002' and 'Q00002 -- Q00001' exist, effectively standarizes to \n",
    "            #     'Q00001 -- Q00002'\n",
    "            edge_id = tuple(sorted([row[1], row[3]]))\n",
    "            edge_ids.add(edge_id)\n",
    "\n",
    "        start_ids = []\n",
    "        start_names = []\n",
    "        end_ids = []\n",
    "        end_names = []\n",
    "\n",
    "\n",
    "        for edge_id in edge_ids:\n",
    "            start_ids.append(edge_id[0])\n",
    "            start_names.append(id_to_name[edge_id[0]])\n",
    "            end_ids.append(edge_id[1])\n",
    "            end_names.append(id_to_name[edge_id[1]])\n",
    "\n",
    "        fixed[kind] = pd.DataFrame({col_names[0]: start_ids, col_names[1]: start_names, col_names[2]: end_ids, col_names[3]: end_names})\n",
    "        \n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_func_numb(node_names, name, func):\n",
    "    return func([wdh.get_query_numb(n) for n in node_names if n.startswith(name)])\n",
    "\n",
    "def find_max_numb(node_names, name):\n",
    "    return find_func_numb(node_names, name, max)\n",
    "\n",
    "def find_min_numb(node_names, name):\n",
    "    return find_func_numb(node_names, name, min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correct_node_name(node_names, name, func):\n",
    "    for node in node_names:\n",
    "        numb = wdh.get_query_numb(node)\n",
    "        if node.startswith(name) and node != name and numb:\n",
    "            return name + str(func(node_names, name))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_and_end_names(node_names, s_type, e_type):\n",
    "\n",
    "    s_name = find_correct_node_name(node_names, s_type, find_min_numb)\n",
    "    e_name = find_correct_node_name(node_names, e_type, find_max_numb)\n",
    "    \n",
    "    return s_name, e_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multi_step_edges(q_result, qualified, multi_step):\n",
    "    fixed = dict()\n",
    "    \n",
    "    # Essentially just change the column order for later processing...\n",
    "    for kind in tqdm_notebook(multi_step.union(qualified)):\n",
    "\n",
    "        # Get the information for the current edge\n",
    "        this_edges = q_result[kind]\n",
    "        col_names = this_edges.columns\n",
    "        node_cols = [c for c in col_names if not c.endswith('Label')]\n",
    "\n",
    "        # Need to know what start and end types we're looking for\n",
    "        s_kind, e_type, e_kind = wdh.gt.parse_edge_abbrev(kind)\n",
    "        s_name = wdh.to_query_name(h.node_abv_to_full[s_kind])[1:]\n",
    "        e_name = wdh.to_query_name(h.node_abv_to_full[e_kind])[1:]\n",
    "\n",
    "        if 'qualifier' not in node_cols:\n",
    "            s_name, e_name = get_start_and_end_names(node_cols, s_name, e_name)\n",
    "\n",
    "        new_node_order = [s_name, e_name] \n",
    "        new_node_order += [n for n in node_cols if n not in new_node_order]\n",
    "\n",
    "        new_col_names = []\n",
    "        for n in new_node_order:\n",
    "            new_col_names += [n, n+'Label']\n",
    "\n",
    "        fixed[kind] = this_edges[new_col_names].copy()\n",
    "        \n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hetnet To Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hetnet_nodes(node_ids, id_to_name):\n",
    "\n",
    "    nodes = []\n",
    "    for k, v in node_ids.items():\n",
    "        curr_nodes = pd.DataFrame({'id': list(v), 'label': len(v)*[k]})\n",
    "        curr_nodes['name'] = curr_nodes['id'].map(id_to_name)\n",
    "        nodes.append(curr_nodes)\n",
    "        \n",
    "    # Make dataframe\n",
    "    nodes = pd.concat(nodes).reset_index(drop=True)\n",
    "    \n",
    "    # Fix labels (from lowercase_underscore to As Defined in node_info.json)\n",
    "    label_map = {wdh.to_query_name(k)[1:]: k for k in h.node_info.keys()}    \n",
    "    nodes['label'] = nodes['label'].map(label_map)\n",
    "        \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Hetnet Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PregBP(edges):\n",
    "    edges_out = edges.copy()\n",
    "\n",
    "    keep_map = {'positive regulation': 'UP_REGULATES_GuBP',\n",
    "                'negative regulation': 'DOWN_REGULATES_GdBP',\n",
    "                'regulation': 'REGULATES_GregBP'}\n",
    "    \n",
    "    direction = edges['biological_process1Label'].str.split(' of ', expand=True)[0]\n",
    "    edges_out['type'] = direction.map(keep_map)\n",
    "    \n",
    "    return edges_out.dropna(subset=['type']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_CpP(edges):\n",
    "    edges_out = edges.copy()\n",
    "    \n",
    "    type_map = {'receptor antagonist': 'INHIBITS_CiG',\n",
    "                'enzyme inhibitor': 'INHIBITS_CiG',\n",
    "                'agonist': 'ACTIVATES_CacG',\n",
    "                'channel blocker': 'INHIBITS_CiG',\n",
    "                'substrate': 'BINDS_CbG',\n",
    "                'allosteric modulator': 'BINDS_CbG',\n",
    "                'channel activator activity': 'ACTIVATES_CacG',\n",
    "                'protein-protein interaction inhibitor': 'INHIBITS_CiG',\n",
    "                'ligand in biochemistry': 'BINDS_CbG',\n",
    "                'reuptake inhibitor': 'INHIBITS_CiG',\n",
    "                'neutralizing antibody': 'INHIBITS_CiG'}\n",
    "    \n",
    "    edges_out['type'] = edges_out['qualifierLabel'].str.lower().map(type_map)\n",
    "    return edges_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hetnet_edges(q_result, fixed_edges):\n",
    "\n",
    "    edges = []\n",
    "\n",
    "    for k, v in q_result.items():\n",
    "        if k in fixed_edges.keys():\n",
    "            v = fixed_edges[k]\n",
    "\n",
    "\n",
    "        col_names = v.columns\n",
    "        keep_cols = [c for c in col_names if not c.endswith('Label')]\n",
    "        \n",
    "        # Queries sometimes return zero results, so skip those...\n",
    "        if not keep_cols:\n",
    "            continue\n",
    "        \n",
    "        col_name_map = {keep_cols[0]: 'start_id', keep_cols[1]: 'end_id'}\n",
    "\n",
    "        # Inner nodes in multi-step edges become inner1, inner2, etc...\n",
    "        inner_cols = {k: 'inner'+str(idx+1) for idx, k in enumerate(keep_cols[2:]) if k != 'qualifier'}\n",
    "        col_name_map = {**inner_cols, **col_name_map}\n",
    "\n",
    "        v = v.rename(columns=col_name_map)\n",
    "\n",
    "        if k == \"PregBP\":\n",
    "            v = process_PregBP(v)\n",
    "        elif k == \"CpP\":\n",
    "            v = process_CpP(v)\n",
    "\n",
    "        # Replace Proteins with Genes, to merge the protein and gene metanodes\n",
    "        parsed_edge = wdh.gt.parse_edge_abbrev(k)\n",
    "        if 'P' in parsed_edge:\n",
    "            idx = parsed_edge.index('P')\n",
    "            parsed_edge = list(parsed_edge)\n",
    "            parsed_edge[idx] = 'G'\n",
    "            k = ''.join(parsed_edge)\n",
    "\n",
    "        if 'type' not in v.columns:\n",
    "            v['type'] = h.edge_abv_to_full[parsed_edge[1]] + '_' + k\n",
    "\n",
    "        edges.append(v)\n",
    "\n",
    "    # Combine the edges into a single dataframe\n",
    "    edges = pd.concat(edges, sort=False).reset_index(drop=True)\n",
    "    col_order = ['start_id', 'end_id', 'type', 'qualifier']\n",
    "    col_order = col_order + [c for c in col_name_map.values() if c not in col_order]\n",
    "    edges = edges[col_order]\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing nodes that are duplicated across two different Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_combos(nodes):\n",
    "    duplicated_nodes = nodes[nodes.duplicated(keep=False, subset=['id'])]['id'].unique()\n",
    "    # Find out what types are being combined...\n",
    "    combos = (nodes.query('id in @duplicated_nodes')\n",
    "                   .sort_values(['id', 'label'])\n",
    "                   .groupby('id')['label']\n",
    "                   .apply(list)\n",
    "                   .astype(str)\n",
    "                   .to_frame()\n",
    "                   .reset_index())\n",
    "    \n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniquify_node_types(nodes, edges, type_fix_map=None, verbose=True):\n",
    "    \n",
    "    # Set a default value for the map\n",
    "    if type_fix_map is None:\n",
    "        \n",
    "        type_fix_map = {\"['Structural Motif', 'Super-Secondary Structure']\": 'Structural Motif',\n",
    "                    \"['Chemical Hazard', 'Disease']\": 'Chemical Hazard',\n",
    "                    \"['Disease', 'Symptom']\": 'Symptom',\n",
    "                    \"['Sequence Variant', 'Symptom']\": 'Symptom',\n",
    "                    \"['Disease', 'Sequence Variant', 'Symptom']\": 'Symptom',\n",
    "                    \"['Compound', 'Gene']\": 'Compound',\n",
    "                    \"['Chemical Role', 'Compound']\": 'Compound',\n",
    "                    \"['Biological Process', 'Disease']\": 'Disease',\n",
    "                    \"['Anatomical Structure', 'Cellular Component']\": 'Cellular Component',\n",
    "                    \"['Protein Domain', 'Structural Motif', 'Super-Secondary Structure']\": 'Protein Domain',\n",
    "                    \"['Protein Domain', 'Protein Family']\": 'Protein Family',\n",
    "                    \"['Gene', 'Protein Family']\": 'Gene',\n",
    "                    \"['Disease', 'Sequence Variant']\": 'Disease'\n",
    "                   }\n",
    "        \n",
    "    # Find out what's combined...\n",
    "    combos = find_combos(nodes)\n",
    "    # Map from the original combination to resolved type\n",
    "    final_types = combos.set_index('id')['label'].map(type_fix_map).to_dict()\n",
    "    \n",
    "    # Fill in types for already unique nodes and map\n",
    "    final_types = {**nodes.set_index('id')['label'].to_dict(), **final_types}\n",
    "    nodes['label'] = nodes['id'].map(final_types)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Number of nodes before fixing: {:,}'.format(len(nodes)))\n",
    "    nodes = nodes.drop_duplicates().reset_index(drop=True)\n",
    "    if verbose:\n",
    "        print('Number of nodes after fixing: {:,}'.format(len(nodes)))\n",
    "\n",
    "    # Now check that the node types in the edge abbreviation match the newly resolved node types \n",
    "    combo = wdh.gt.combine_nodes_and_edges(nodes, edges)\n",
    "    \n",
    "    combo['edge_abv'] = combo['type'].apply(lambda t: t.split('_')[-1])\n",
    "    combo['actual_start'] = combo['edge_abv'].apply(lambda a: h.node_abv_to_full[wdh.gt.parse_edge_abbrev(a)[0]])\n",
    "    combo['actual_end'] = combo['edge_abv'].apply(lambda a: h.node_abv_to_full[wdh.gt.parse_edge_abbrev(a)[2]])\n",
    "    \n",
    "    bad_edge = combo.query('start_label != actual_start or end_label != actual_end')\n",
    "    \n",
    "    if verbose:\n",
    "        print('Number of edges with issues to be removed: {:,}'.format(len(bad_edge)))\n",
    "        print('Number of edges before fixing: {:,}'.format(len(edges)))\n",
    "    \n",
    "    edges = edges.drop(bad_edge.index).reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of edges after fixing: {:,}'.format(len(edges)))\n",
    "\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hetnet(q_result):\n",
    "    node_ids, id_to_name, self_ref, qualified, multi_step = process_query_res(q_result)\n",
    "    \n",
    "    fixed_self_ref = fix_self_ref_edges(q_result, self_ref, id_to_name)\n",
    "    fixed_multi_step = process_multi_step_edges(q_result, qualified, multi_step)\n",
    "    \n",
    "    nodes = build_hetnet_nodes(node_ids, id_to_name)\n",
    "    edges = build_hetnet_edges(q_result, {**fixed_multi_step, **fixed_self_ref})\n",
    "    \n",
    "    # merge the genes and proteins in the nodes file\n",
    "    idx = nodes.query('label == \"Protein\"').index\n",
    "    nodes.loc[idx, 'label'] = 'Gene'\n",
    "\n",
    "    nodes, edges = uniquify_node_types(nodes, edges)\n",
    "    \n",
    "    return nodes, edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMP DATE: 2018-11-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c75e3f1d9c941ca865ba0bf3d50f637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98c26d712c24042947e4842fbc7f298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes before fixing: 88,404\n",
      "Number of nodes after fixing: 88,076\n",
      "Number of edges with issues to be removed: 38,029\n",
      "Number of edges before fixing: 582,565\n",
      "Number of edges after fixing: 544,536\n",
      "\n",
      "\n",
      "\n",
      "DUMP DATE: 2018-02-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826e1a4ea1984d539f98c5766a09bc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496fec99d7ac4f088c715b8b4c39eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes before fixing: 66,444\n",
      "Number of nodes after fixing: 66,139\n",
      "Number of edges with issues to be removed: 37,289\n",
      "Number of edges before fixing: 510,202\n",
      "Number of edges after fixing: 472,913\n",
      "\n",
      "\n",
      "\n",
      "DUMP DATE: 2017-01-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c77ac73ca54ce4ad2a110983a76115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d1b80f96ac476183619dd4db6ee9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes before fixing: 42,389\n",
      "Number of nodes after fixing: 42,355\n",
      "Number of edges with issues to be removed: 370\n",
      "Number of edges before fixing: 135,576\n",
      "Number of edges after fixing: 135,206\n",
      "\n",
      "\n",
      "\n",
      "DUMP DATE: 2019-09-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec725d4b8844aaf91d8504a1d35ad70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34082befde0a4c42a7b7bf4982860b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes before fixing: 96,423\n",
      "Number of nodes after fixing: 96,305\n",
      "Number of edges with issues to be removed: 4,322\n",
      "Number of edges before fixing: 722,084\n",
      "Number of edges after fixing: 717,762\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for date, q_result in results.items():\n",
    "    out_dir.joinpath(date).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print('DUMP DATE: {}'.format(date))\n",
    "    \n",
    "    nodes, edges = build_hetnet(q_result)\n",
    "\n",
    "    wdh.gt.add_colons(nodes).to_csv(out_dir.joinpath(date, 'nodes.csv'), index=False)\n",
    "    wdh.gt.add_colons(edges).to_csv(out_dir.joinpath(date, 'edges.csv'), index=False)\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
